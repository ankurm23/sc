{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Study of Fuzzy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10, 11, 12, 13, 14, 15}\n",
      "{11, 12, 13}\n",
      "{14, 15}\n"
     ]
    }
   ],
   "source": [
    "A = {10,11,12,13}\n",
    "B = {11,12,13,14,15}\n",
    "\n",
    "# UNION\n",
    "Union = A.union(B)\n",
    "print(Union) #{10, 11, 12, 13, 14, 15}\n",
    "\n",
    "# INTERSECTION\n",
    "Intersection = A.intersection(B)\n",
    "print(Intersection) #{11, 12, 13}\n",
    "\n",
    "# COMPLEMENT\n",
    "Complement = B - A\n",
    "print(Complement) #{14, 15}\n",
    "\n",
    "# ALGEBRIC SUM\n",
    "A = {0.2/1 + 0.3/2 + 0.4/3 + 0.5/4}\n",
    "B = {0.1/1 + 0.2/2 + 0.2/3 + 0/4}\n",
    "µ(A+B) = µ(A) + µ(B) - (µ(A) * µ(B))\n",
    "Algebricsum = {0.3/1 + 0.5/2 + 0.6/3 + 0.5/4} - {0.02/1 + 0.06/2 + 0.08/3 + 0/4}\n",
    "Algebricsum = {0.28/1 + 0.44/2 + 0.52/3 + 0.5/4}\n",
    "\n",
    "# ALGEBRIC PRODUCT\n",
    "A = {0.2/1 + 0.3/2 + 0.4/3 + 0.5/4}\n",
    "B = {0.1/1 + 0.2/2 + 0.2/3 + 0/4}\n",
    "µ(A*B) = (µ(A) * µ(B))\n",
    "AlgebricProduct = {0.02/1 + 0.06/2 + 0.08/3 + 0/4}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Implementation of Fuzzy Relation (Max-Min Composition)\n",
    "\n",
    "Max-min composition is a method ofcombining two fuzzy relations to create a new \n",
    "fuzzy relation. It is a common operation in fuzzy logic and is used to model \n",
    "complex relationships between fuzzy sets. \n",
    "Given two fuzzy relations R and S defined on the same universe of discourse X, \n",
    "the max-min composition of R and S, denoted by R∘S, is a fuzzy relation on X×X \n",
    "defined as follows: \n",
    "(R∘S)(x, z) = max(min(R(x, y), S(y, z))) for all x, z ϵ X, where y ϵ X. \n",
    "Intuitively, the max-min composition of R and S can be thought of as a \"transitive \n",
    "closure\" of R and S. That is, if R (x, y) and S(y, z) are both high (i.e., close to 1), \n",
    "then the composition (R∘S)(x, z) will also be high. Conversely, if either R (x, y) or \n",
    "S (y, z) is low (i.e., close to 0), then the composition (R∘S)(x, z) will be low. \n",
    "One important property of max-min composition is that it is associative, meaning \n",
    "that (R∘S) ∘T = R∘(S∘T) for all fuzzy relations R, S, and T. This property allows us \n",
    "to chain together multiple fuzzy relations to model more complex relationships. \n",
    "In practice, the max-min composition of two fuzzy relations can be computed by \n",
    "taking the minimum of the maximum of the minimum values of the corresponding \n",
    "rows of the two relations. This is often implemented using matrix multiplication or \n",
    "other efficient algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max-Min Composition:\n",
      " [[0.5 0.3 0.3]\n",
      " [0.6 0.5 0.7]\n",
      " [0.8 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = [\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.1, 0.7, 0.6],\n",
    "    [0.3, 0.5, 0.8]\n",
    "    ]\n",
    "\n",
    "B = [\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.2, 0.5, 0.8],\n",
    "    [0.8, 0.1, 0.5]\n",
    "    ]\n",
    "\n",
    "A = np.array(A)\n",
    "B = np.array(B)\n",
    "\n",
    "T = np.zeros((len(A),len(B)))\n",
    "\n",
    "for i in range(len(A)):\n",
    "    for j in range(len(B)):\n",
    "        T[i,j] = np.max(np.minimum(A[i,:], B[:,j]))\n",
    "\n",
    "print(\"Max-Min Composition:\\n\",T)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Implementation of Simple Neural Network\n",
    "\n",
    "The McCulloch-Pitts model is a simple model of a neuron, proposed by Warren McCulloch and \n",
    "Walter Pitts in 1943. It is a simplified model of a biological neuron and is based on the idea that \n",
    "a neuron receives inputs from other neurons or sensory organs and produces an output based on \n",
    "these inputs. \n",
    "The McCulloch-Pitts model consists of a set of binary inputs, a set of weights associated with \n",
    "each input, and a threshold. The neuron computes the weighted sum of the inputs and compares it \n",
    "to the threshold. If the weighted sum is greater than or equal to the threshold, the neuron \n",
    "produces an output of 1. Otherwise, it produces an output of 0. \n",
    "Formally, let x1, x2, ..., xn be the binary inputs w1, w2, ..., wn be the weights associated with \n",
    "each input, and θ be the threshold. Then the output y of the neuron is given by: \n",
    "y = 1 if ∑(xi * wi) >= θ\n",
    " 0 otherwise \n",
    "Where ∑(xi * wi) represents the weighted sum of the inputs.\n",
    "The McCulloch-Pitts model is a building block for more complex neural networks and has been \n",
    "used in various applications such as pattern recognition, signal processing, and control systems. \n",
    "It is a simple yet powerful model that can capture complex nonlinear relationships between \n",
    "inputs and outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Final Output is 1\n"
     ]
    }
   ],
   "source": [
    "def threshold(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def mcCulloh_pitts(inputs,weights):\n",
    "    weighted_sum = sum([inputs[i]*weights[i] for i in range(len(inputs))])\n",
    "    output = threshold(weighted_sum)\n",
    "    return output\n",
    "\n",
    "inputs = [1,0,1]\n",
    "weights = [0.5,-0.5,0.5]\n",
    "output = mcCulloh_pitts(inputs,weights)\n",
    "print(\"The Final Output is {}\".format(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Implement Single Layer Perceptron\n",
    "\n",
    "The perceptron learning rule can be summarized as follows:\n",
    "\n",
    "1. Initialize the weights of the network to small random values. 2. For each training example:\n",
    "\n",
    "a. Compute the output of the network based on the current weights. b. Compare the predicted output to the actual output.\n",
    "\n",
    "c. If the predicted output is incorrect, adjust the weights of the network in the direction of the correct output.\n",
    "\n",
    "3. Repeat step 2 until the network achieves a satisfactory level of performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n",
      "0 AND 0 = [0]\n",
      "0 AND 1 = [0]\n",
      "1 AND 0 = [0]\n",
      "1 AND 1 = [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "clf = Perceptron()\n",
    "clf.fit(X, y)\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "predictions = clf.predict(test_data)\n",
    "print(predictions)\n",
    "\n",
    "# Scratch\n",
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Output data\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Step function\n",
    "def step_function(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Initialize weights and bias\n",
    "W = np.random.rand(2, 1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Training the model\n",
    "for i in range(10000):\n",
    "    # Forward propagation\n",
    "    Z = np.dot(X, W) + b\n",
    "    A = step_function(Z)\n",
    "\n",
    "    # Backward propagation\n",
    "    error = Y - A\n",
    "    dW = lr * np.dot(X.T, error)\n",
    "    db = lr * np.sum(error)\n",
    "\n",
    "    # Update weights and bias\n",
    "    W += dW\n",
    "    b += db\n",
    "\n",
    "# Testing the model\n",
    "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "for x in test_data:\n",
    "    result = step_function(np.dot(x, W) + b)\n",
    "    print(f\"{x[0]} AND {x[1]} = {result}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Apply Hebbs and Delta Rule\n",
    "\n",
    "Hebbian Learning Rule Algorithm\n",
    "\n",
    "1. Set all weights to re 0 for inlaid base\n",
    "\n",
    "2. For each input vector, S (oput vector) target output pair, repeat steps 3-1 3. Set activations for input with the input vector X 4. Set the corresponding output to the outpist for 1-1 to a\n",
    "\n",
    "5 Update weight and bias by applying Hebb role for all (-1 to a\n",
    "\n",
    "w, (now)=w, (old)+xy\n",
    "\n",
    "b(new)= b (old)+y\n",
    "\n",
    "The Delta learing pervised learning algorithm used for updating the wigs of neural\n",
    "\n",
    "setwork during the tiny peces 50 90\n",
    "\n",
    "NBX The Delta leuning enged to mimire the error between the posted it off work and the actual output. It does this by adjusting die-waights of the network in the direction it reduces the erre\n",
    "\n",
    "The Delta learning rule work follows:\n",
    "\n",
    "1. The network receives an input Vectothich is malplied by the weights to produce the output 2. The output is compared to the actual output to calculate the enor 3. The weights are adjusted based on the error and the learning rate. The learning rate is a param\n",
    "\n",
    "that determines the size of the weight update 4. The process is repeatest for each input Vector in the training se\n",
    "\n",
    "In Mathematical in the delix nile is as follows\n",
    "\n",
    "delta w = n (t-y) x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77955031 0.22553648 0.98385229]\n",
      " [0.26299372 0.32367958 0.69123334]\n",
      " [0.84606871 0.74841357 0.25119327]]\n"
     ]
    }
   ],
   "source": [
    "# Hebbs Rule\n",
    "import numpy as np\n",
    "X = np.array([[1,0,0],[0,1,0],[0,0,1],[-1,0,0],[0,-1,0],[0,0,-1]])\n",
    "W = np.random.rand(3,3)\n",
    "lr = 0.1\n",
    "for i in range(X.shape[0]):\n",
    "    x = X[i].reshape(-1,1)\n",
    "    W += lr * np.dot(x,x.T)\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47757617]\n",
      " [0.94216636]\n",
      " [0.5126008 ]]\n"
     ]
    }
   ],
   "source": [
    "# Delta Rule\n",
    "X = np.array([[1,0,0],[0,1,0],[0,0,1],[-1,0,0],[0,-1,0],[0,0,-1]])\n",
    "Y = np.array([[1],[1],[1],[-1],[-1],[-1],])\n",
    "W = np.random.rand(3,1)\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    x = X[i].reshape(-1,1)\n",
    "    error = Y[i] - np.dot(W.T,x)\n",
    "    W += lr * error * x\n",
    "print(W)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Write a program for Error Back Propogation Algorithm(EBPA) Learning\n",
    "\n",
    "Step 1: Inputs X, arrive through the preconnected path. \n",
    "Step 2: The input is modeled using true weights W. Weights are usually chosen randomly. \n",
    "Step 3: Calculate the output of each neuron from the input layer to the hidden layer to the output layer. \n",
    "Step 4: Calculate the error in the outputs \n",
    "Backpropagation Error= Actual Output – Desired Output \n",
    "Step 5: From the output layer, go back to the hidden layer to adjust the weights to reduce the error. \n",
    "Step 6: Repeat the process until the desired output is achieved. \n",
    "• x = inputs training vector x=(x1,x2,…………xn). \n",
    "• t = target vector t=(t1,t2……………tn).\n",
    "• δk = error at output unit. \n",
    "• δj = error at hidden layer. \n",
    "• α = learning rate. \n",
    "• V0j = bias of hidden unit j. Training Algorithm :\n",
    "Step 1: Initialize weight to small random values. \n",
    "Step 2: While the steps stopping condition is to be false do step 3 to 10. \n",
    "Step 3: For each training pair do step 4 to 9 (Feed-Forward). \n",
    "Step 4: Each input unit receives the signal unit and transmits the signal xi signal to all the units. \n",
    "Step 5 : Each hidden unit Zj (z=1 to a) sums its weighted input signal to calculate its net input \n",
    "zinj = v0j + Σxivij ( i=1 to n) \n",
    " Applying activation function zj = f(zinj) and sends this signals to all units in the layer about i.e \n",
    "output units \n",
    " For each output l=unit yk = (k=1 to m) sums its weighted input signals. \n",
    " yink = w0k + Σ ziwjk (j=1 to a) and applies its \n",
    "activation function to calculate the output signals. yk = \n",
    "f(yink) \n",
    "Backpropagation Error :\n",
    "Step 6: Each output unit yk (k=1 to n) receives a target pattern corresponding to an input pattern then \n",
    "error is calculated as: \n",
    " δk = ( tk – yk ) + yink \n",
    "Step 7: Each hidden unit Zj (j=1 to a) sums its input from all units in the layer above \n",
    "δinj = Σ δj wjk \n",
    " The error information term is calculated as : \n",
    "δj = δinj + zinj\n",
    "Updation of weight and bias :\n",
    "Step 8: Each output unit yk (k=1 to m) updates its bias and weight (j=1 to a). The weight correction term \n",
    "is given by : \n",
    " Δ wjk = α δk zj and the bias \n",
    "correction term is given by Δwk = α δk.\n",
    " therefore wjk(new) = wjk(old) + Δ wjk\n",
    " w0k(new) = wok(old) + Δ wok\n",
    " for each hidden unit zj (j=1 to a) update its bias and weights (i=0 to n) the weight connection term \n",
    " Δ vij = α δj xi \n",
    "and the bias connection on term \n",
    " Δ v0j = α δj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72128524]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define derivative of sigmoid activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define training data\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10000\n",
    "num_hidden_units = 4\n",
    "\n",
    "# Initialize weights\n",
    "input_dim = X.shape[1]\n",
    "output_dim = y.shape[1]\n",
    "\n",
    "W1 = np.random.randn(input_dim, num_hidden_units)\n",
    "b1 = np.zeros((1, num_hidden_units))\n",
    "\n",
    "W2 = np.random.randn(num_hidden_units, output_dim)\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "# Train the network using backpropagation\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # Compute error\n",
    "    error = y - a2\n",
    "    \n",
    "    # Backward pass\n",
    "    delta2 = error * sigmoid_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, delta2)\n",
    "    db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "    \n",
    "    delta1 = np.dot(delta2, W2.T) * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, delta1)\n",
    "    db1 = np.sum(delta1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Update weights\n",
    "    W2 += learning_rate * dW2\n",
    "    b2 += learning_rate * db2\n",
    "    W1 += learning_rate * dW1\n",
    "    b1 += learning_rate * db1\n",
    "    \n",
    "# Predict output for new input\n",
    "new_input = np.array([[1, 0, 0]])\n",
    "z1 = np.dot(new_input, W1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "z2 = np.dot(a1, W2) + b2\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(a2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Design a fuzzy controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# Define input variables\n",
    "water_level = ctrl.Antecedent(np.arange(0, 11, 1), 'water_level')\n",
    "dirt_level = ctrl.Antecedent(np.arange(0, 11, 1), 'dirt_level')\n",
    "\n",
    "# Define output variable\n",
    "detergent_amount = ctrl.Consequent(np.arange(0, 11, 1), 'detergent_amount')\n",
    "\n",
    "# Define membership functions for input variables\n",
    "water_level['low'] = fuzz.trimf(water_level.universe, [0, 0, 5])\n",
    "water_level['medium'] = fuzz.trimf(water_level.universe, [0, 5, 10])\n",
    "water_level['high'] = fuzz.trimf(water_level.universe, [5, 10, 10])\n",
    "\n",
    "dirt_level['low'] = fuzz.trimf(dirt_level.universe, [0, 0, 5])\n",
    "dirt_level['medium'] = fuzz.trimf(dirt_level.universe, [0, 5, 10])\n",
    "dirt_level['high'] = fuzz.trimf(dirt_level.universe, [5, 10, 10])\n",
    "\n",
    "# Define membership functions for output variable\n",
    "detergent_amount['low'] = fuzz.trimf(detergent_amount.universe, [0, 0, 5])\n",
    "detergent_amount['medium'] = fuzz.trimf(detergent_amount.universe, [0, 5, 10])\n",
    "detergent_amount['high'] = fuzz.trimf(detergent_amount.universe, [5, 10, 10])\n",
    "\n",
    "# Define fuzzy rules\n",
    "rule1 = ctrl.Rule(water_level['low'] & dirt_level['low'], detergent_amount['low'])\n",
    "rule2 = ctrl.Rule(water_level['low'] & dirt_level['medium'], detergent_amount['low'])\n",
    "rule3 = ctrl.Rule(water_level['low'] & dirt_level['high'], detergent_amount['medium'])\n",
    "rule4 = ctrl.Rule(water_level['medium'] & dirt_level['low'], detergent_amount['low'])\n",
    "rule5 = ctrl.Rule(water_level['medium'] & dirt_level['medium'], detergent_amount['medium'])\n",
    "rule6 = ctrl.Rule(water_level['medium'] & dirt_level['high'], detergent_amount['high'])\n",
    "rule7 = ctrl.Rule(water_level['high'] & dirt_level['low'], detergent_amount['medium'])\n",
    "rule8 = ctrl.Rule(water_level['high'] & dirt_level['medium'], detergent_amount['high'])\n",
    "rule9 = ctrl.Rule(water_level['high'] & dirt_level['high'], detergent_amount['high'])\n",
    "\n",
    "# Define control system\n",
    "washing_machine_ctrl = ctrl.ControlSystem(\n",
    "    [rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n",
    "\n",
    "# Define simulation\n",
    "washing_machine_sim = ctrl.ControlSystemSimulation(washing_machine_ctrl)\n",
    "\n",
    "# Set input values\n",
    "washing_machine_sim.input['water_level'] = 3\n",
    "washing_machine_sim.input['dirt_level'] = 7\n",
    "\n",
    "# Run simulation\n",
    "washing_machine_sim.compute()\n",
    "\n",
    "# Print output value\n",
    "print(washing_machine_sim.output['detergent_amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.22.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-fuzzy\n",
      "  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n",
      "Requirement already satisfied: numpy>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-fuzzy) (1.22.4)\n",
      "Requirement already satisfied: scipy>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-fuzzy) (1.7.3)\n",
      "Requirement already satisfied: networkx>=1.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-fuzzy) (2.7.1)\n",
      "Building wheels for collected packages: scikit-fuzzy\n",
      "  Building wheel for scikit-fuzzy (setup.py): started\n",
      "  Building wheel for scikit-fuzzy (setup.py): finished with status 'done'Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894089 sha256=9374a7294ed976eecab0710280a197ae633bf39fdde77ba8d718d9709174f780\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\32\\2c\\a1\\a90a7d7dd8448ec029f298a61f3490275e99b17aa348be675c\n",
      "Successfully built scikit-fuzzy\n",
      "Installing collected packages: scikit-fuzzy\n",
      "Successfully installed scikit-fuzzy-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-fuzzy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
